{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import save_submission,load_data\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "CLASSES = 10 # note: could have inferred this automatically from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Convert a set of scores from a linear model into a softmax posterior\n",
    "    Input:\n",
    "    Z: N x C array of scores\n",
    "        Z[n][:] is the set of scores for C classes for the n-th example\n",
    "    Output:\n",
    "    S: N x C array\n",
    "        S[n][:] is the softmax distribution over C classes for the n-th example\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "\n",
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : dictionary containing 'weight' and 'bias'\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the softmax posterior distribution over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return softmax(np.dot(X, model['weight']) + model['bias'])\n",
    "\n",
    "def test(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute hard label assignments based on model predictions, and return the accuracy vector\n",
    "    Input:\n",
    "    X : N x d array of data (no constant term)\n",
    "    Y : N x C array with 1-hot encoding of true labels\n",
    "    model: dictionary \n",
    "    Output:\n",
    "    acc : N array of errors, acc[n] is 1 if correct and 0 otherwise\n",
    "    \"\"\"\n",
    "    return predict(X, model).argmax(-1) == Y.argmax(-1)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    return 1 - test(X, Y, model).mean()\n",
    "\n",
    "def calc_loss(X, Y, model):\n",
    "    \"\"\"\n",
    "    Evaluate the loss (without regularization penalty), i.e., normalized negative log likelihood\n",
    "    \"\"\"\n",
    "    Z = predict(X, model)\n",
    "    return -(Y * np.log(Z)).sum() / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcGrad(X, Y, model):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the loss w.r.t. model parameters\n",
    "    Output:\n",
    "    grad_W : same dimension as model['weight']; gradient w.r.t. the weights\n",
    "    grad_b : same dimension as model['bias']; gradient w.r.t. the bias terms\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \n",
    "    return grad_W, grad_b\n",
    "\n",
    "def modelUpdate(X, Y, model, lambda_, eta):\n",
    "    \"\"\"\n",
    "    Update the model\n",
    "    Input:\n",
    "    X, Y : the inputs and 1-hot encoded labels\n",
    "    model : the currrent model\n",
    "    lambda : regularization coefficient for L2 penalty\n",
    "    eta : learning rate\n",
    "    Output:\n",
    "    updated model\n",
    "    \"\"\"\n",
    "    grad_W,grad_b = calcGrad(X,Y,model)\n",
    "    \n",
    "    YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    \"\"\"\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        bX = X[shuffled_idx[batch_idx],:]\n",
    "        bY = Y[shuffled_idx[batch_idx],:]\n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            print('{:8} batch loss: {:.3f}'.format(iteration, calc_loss(bX, bY, model)))\n",
    "        model = modelUpdate(bX, bY, model, trainopt['lambda'], eta)\n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "    # compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "    trainError = 100 * error_rate(X, Y, model)\n",
    "    valError = 100 * error_rate(Xval, Yval, model)\n",
    "    \n",
    "    return model,valError,trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fn = \"NOISY_MNIST_SUBSETS.h5\"\n",
    "\n",
    "print(\"Load small train.\")\n",
    "Xsmall,Ysmall = load_data(data_fn, \"small_train\")\n",
    "print(Xsmall.shape)\n",
    "print(Ysmall.shape)\n",
    "\n",
    "print(\"Load large train.\")\n",
    "Xlarge,Ylarge = load_data(data_fn, \"large_train\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load val.\")\n",
    "Xval,Yval = load_data(data_fn, \"val\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- display first ten small train examples\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(Xsmall[i,:].reshape(24,24),cmap='gray')\n",
    "    plt.title(repr(Ysmall[i].argmax(-1)))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- display first ten val examples\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(Xval[i,:].reshape(24,24),cmap='gray')\n",
    "    plt.title(repr(Yval[i].argmax(-1)))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- training options; these are suggestions, feel free to experiment\n",
    "trainopt = {\n",
    "    'eta': 1,   # initial learning rate\n",
    "    'maxiter': 100000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 20000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 10,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .2  # drop eta every eta_frac fraction of the max iterations\n",
    "                    # so if eta_frac is .2, and maxiter is 10000, drop eta every 2000 iterations\n",
    "}\n",
    "\n",
    "NFEATURES = Xsmall.shape[1]\n",
    "\n",
    "seed = np.random.RandomState(2341)  # to make sure everyone starts from the same point\n",
    "random_init = seed.normal(scale=0.01, size=(NFEATURES,CLASSES)) # -- with random seed fixed\n",
    "\n",
    "model = { 'weight': random_init, 'bias': np.zeros(CLASSES) }\n",
    "\n",
    "# set the (initial?) set of lambda values to explore\n",
    "lambdas = np.array(ENTER REASONABLE VALUES HERE, UPDATE/REFINE AS NEEDED)\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "small_trained_models = dict()\n",
    "large_trained_models = dict()\n",
    "\n",
    "# -- small train set: sweep the lambda values\n",
    "for lambda_ in lambdas:\n",
    "    trainopt['lambda'] = lambda_\n",
    "    # -- model trained on small train set\n",
    "    # note: you need deepcopy here because of the way Python handles copying of complex objects like dictionaries\n",
    "    # by default, it would copy it by reference, i.e., it would make a new pointer to the same data, so later changing\n",
    "    # the contents would change the \"copied\" version as well. deepcopy actually makes a copy.\n",
    "    trained_model,valErr,trainErr = runTrainVal(Xsmall, Ysmall, copy.deepcopy(model), Xval, Yval, trainopt)\n",
    "    small_trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "    print('small train set model: -> lambda= %.4f, train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    \n",
    "    \n",
    "for lambda_ in lambdas:\n",
    "    trainopt['lambda'] = lambda_\n",
    "    # -- model trained on large train set\n",
    "    trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, copy.deepcopy(model), Xval, Yval, trainopt)\n",
    "    large_trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "    print('large train set model: -> lambda= %.4f, train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    \n",
    "best_small_trained_lambda = 0.\n",
    "best_small_trained_model = None\n",
    "best_small_trained_val_err = 100.\n",
    "for lambda_,results in small_trained_models.items():\n",
    "    if results['val_err'] < best_small_trained_val_err:\n",
    "        best_small_trained_val_err = results['val_err']\n",
    "        best_small_trained_model = results['model']\n",
    "        best_small_trained_lambda = lambda_\n",
    "        \n",
    "best_large_trained_lambda = 0.\n",
    "best_large_trained_model = None\n",
    "best_large_trained_val_err = 100.\n",
    "for lambda_,results in large_trained_models.items():\n",
    "    if results['val_err'] < best_large_trained_val_err:\n",
    "        best_large_trained_val_err = results['val_err']\n",
    "        best_large_trained_model = results['model']\n",
    "        best_large_trained_lambda = lambda_\n",
    "\n",
    "print(\"Best small train model val err:\", best_small_trained_val_err)\n",
    "print(\"Best small train model lambda:\", best_small_trained_lambda)\n",
    "print(\"Best large train model val err:\", best_large_trained_val_err)\n",
    "print(\"Best large train model lambda:\", best_large_trained_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Generate a Kaggle submission file using `model`\n",
    "kaggleX = load_data(data_fn, 'kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_large_trained_model).argmax(-1)\n",
    "save_submission('submission-large.csv', kaggleYhat)\n",
    "\n",
    "# same for small training set\n",
    "kaggleYhat = predict(kaggleX, best_large_trained_model).argmax(-1)\n",
    "save_submission('submission-small.csv', kaggleYhat)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
