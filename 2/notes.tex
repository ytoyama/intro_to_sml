\documentclass{introtosml}
\nth{2}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}

\begin{document}

\maketitle

\begin{p}
  \item
    \begin{gather*}
      \begin{aligned}
        R(h_r; q)
        & = \intx{\sum_{c=1}^C \sum_{c'=1}^C L_{0/1}(c', c) \q p(\vec{x}, y = c)} \\
        & = \intx{\R{h_r} p(\vec{x})}
      \end{aligned} \\
      \begin{aligned}
        \text{where} \R{h_r}
        & = \sum_{c=1}^C \sum_{c'=1}^C L_{0/1} (c', c) \q \pyxc \\
        & = \sum_{c=1}^C \sum_{c' \ne c}^C \q \pyxc \\
        & = \sum_{c=1}^C (1 - \qc) \pyxc
      \end{aligned} \\
      \begin{aligned}
        R(h^*) & = \intx{\R{h^*} p(\vec{x})}
      \end{aligned} \\
      \begin{aligned}
        \text{where} \R{h^*}
        & = \sum_{c=1}^C L_{0/1} (h^*(\vec{x}), c) \pyxc \\
        & = \sum_{c \ne h^*}^C \pyxc \\
        & = 1 - \pyxh
      \end{aligned} \\
      \begin{aligned}
        \R{h_r} - \R{h^*}
        & = \sum_{c=1}^C (1 - \qc) \pyxc - (1 - \pyxh) \\
        & = \pyxh - \sum_{c=1}^C \qc \pyxc \\
        & = \sum_{c=1}^C \qc (\pyxh - \pyxc) \\
        & \ge 0
      \end{aligned} \\
      \begin{aligned}
        \therefore \R{h_r} & \ge \R{h^*} \\
        \therefore R(h_r; q) & \ge R(h^*)
      \end{aligned} \\
    \end{gather*}

  \item
    Let $M$ be the number of augmented data points.
    \begin{gather*}
      \sum_{i=1}^{N+M} (y_i - \w^T \x_i)^2
          = \sum_{i=1}^{N} (y_i - \w^T \x_i)^2 + \lambda \norm{\w}^2 \\
      \sum_{i=N+1}^{N+M} (y_i - \w^T \x_i)^2 = \lambda \norm{\w}^2
    \end{gather*}
    Let $y_i = 0$ and $\x_i = [0, a, \ldots, a]^T $.
    \begin{gather*}
      \begin{aligned}
        \sum_{i=N+1}^{N+M} a^2 \norm{\w}^2 & = \lambda \norm{\w}^2 \\
        M a^2 \norm{\w}^2 & = \lambda \norm{\w}^2 \\
        M a^2 & = \lambda
      \end{aligned} \\
      \therefore \vec{y}' = \left[\begin{array}{cccc}
        y_1 \\ \vdots \\ y_N \\ 0 \\ \vdots \\ 0
      \end{array}\right],
      X' = \left[\begin{array}{cccc}
        1 & x_{11} & \cdots & x_{1d} \\
        \vdots & \vdots & \vdots & \vdots  \\
        1 & x_{N1} & \cdots & x_{Nd} \\
        0 & a & \vdots & a \\
        \vdots & \vdots & \vdots & \vdots  \\
        0 & a & \vdots & a \\
      \end{array}\right] \\
      \st Ma^2 = \lambda \text{(where $M$ is the number of augmented data points)}
    \end{gather*}

  \item
    \begin{align*}
      \forall i, j, \log \frac{p(c_i|x)}{p(c_j|x)} & = \w_{ij} \x
      \log p(c_i|\x) - \log p(c_j|\x) & = \w_{ij} \x
    \end{align*}
    Let $\log p(c_i|x) = \w_i \x$.
    \begin{align*}
      \w_i \x - \w_j \x & = \w_{ij} \x \\
      \w_{ij} & = \w_i - \w_j
    \end{align*}
    \begin{align*}
      p(c_i|\x) = e^{\w_{ij} \x} p(c_j|\x) \\
      p(c_i|\x) = e^{\w_i \x} e^{- \w_j \x} p(c_j|\x) \\
      1 = \sum_{i=1}^C e^{\w_i \x} e^{- \w_j \x} p(c_j|\x) \\
      p(c_j|\x) = \frac{e^{\w_j \x}}{\sum_{i=1}^C e^{\w_i \x}}
    \end{align*}
    \therefore the softmax model corresponds to modeling the log-odds between any two classes.
\end{p}

\end{document}
