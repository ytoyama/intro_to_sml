{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the preprocessed data:\n",
    "X and y are dicts, has keys train, val and test. keys gives you what word (or pair of words) each dimension corresponds to.\n",
    "\n",
    "You can also get bigram features if you call preprocess(bigram = True)`\n",
    "Another optional argument mincount specifies the frequency cutoff for inclusion of a word/bigram in the\n",
    "dictionary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X, y, keys = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    \"\"\"\n",
    "    A class containing all kinds of kernels.\n",
    "    Note: the kernel should work for both input (Matrix, vector) and (vector, vector)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(x, y):\n",
    "            return np.dot(x, y)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(gamma):  # we use the commonly used name, although it's not really a Gaussian\n",
    "        def f(x, y):\n",
    "            exponent = - gamma * np.linalg.norm((x-y).transpose(), 2, 0) ** 2\n",
    "            return np.exp(exponent)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def _poly(dimension, offset):\n",
    "        def f(x, y):\n",
    "            return (offset + np.dot(x, y)) ** dimension\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def inhomogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def homogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        def f(x, y):\n",
    "            return np.tanh(kappa * np.dot(x, y) + c)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    def __init__(self, kernel, c):\n",
    "        \"\"\"\n",
    "        Build a SVM given kernel function and C\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kernel : function\n",
    "            a function takes input (Matrix, vector) or (vector, vector)\n",
    "        c : a scalar\n",
    "            balance term\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        self._kernel = kernel\n",
    "        self._c = c\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model given data X and ground truth label y\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        # Solve the QP problem to get the multipliers\n",
    "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
    "        # Get all the support vectors, support weights and bias\n",
    "        self._construct_predictor(X, y, lagrange_multipliers)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the label given data X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1D array\n",
    "            predicted label\n",
    "        \"\"\"\n",
    "        kernel = np.concatenate([self._kernel(self._support_vectors, X[i]).reshape(1, len(self._support_vectors))\n",
    "                                 for i in range(len(X))])\n",
    "        return np.sign(np.einsum(\"i,i,ji->j\", self._weights, self._support_vector_labels, kernel) + self._bias)\n",
    "\n",
    "    def _kernel_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Get the kernel matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        K : 2D array\n",
    "            N x N kernel matrix\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "        K = np.zeros((N, N))\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                K[i, j] = self._kernel(x_i, x_j)\n",
    "        return K\n",
    "\n",
    "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        \"\"\"\n",
    "        Given the data, label and the multipliers, extract the support vectors and calculate the bias\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "        lagrange_multipliers: 1D array\n",
    "            the solution of lagrange_multiplier\n",
    "\n",
    "        Fills in relevant variables: model bias and weights (alphas), and details of support vectors\n",
    "        -------\n",
    "        \"\"\"\n",
    "        support_vector_indices = lagrange_multipliers > 1e-5\n",
    "            \n",
    "        print(\"SV number: \", np.sum(support_vector_indices))\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        \"\"\"\n",
    "        Get the bias term (w_0)\n",
    "        \"\"\"\n",
    "        for i, mult in enumerate(support_multipliers):\n",
    "            if mult < self._c * 0.99:\n",
    "                break\n",
    "\n",
    "        bias = support_vector_labels[i] - np.einsum(\"i,i,ij,j\",\n",
    "                                                    support_multipliers,\n",
    "                                                    support_vector_labels,\n",
    "                                                    support_vectors,\n",
    "                                                    support_vectors[i])\n",
    "\n",
    "        self._bias=bias\n",
    "        self._weights=support_multipliers\n",
    "        self._support_vectors=support_vectors\n",
    "        self._support_vector_labels=support_vector_labels\n",
    "\n",
    "    def _compute_multipliers(self, X, y):\n",
    "        \"\"\"\n",
    "        Given the data, label, solve the QP program to get lagrange multiplier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "            N x d data matrix (row per example)\n",
    "        y : 1D array\n",
    "            class label\n",
    "\n",
    "        Returns\n",
    "        lagrange_multipliers: 1D array\n",
    "        -------\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "\n",
    "        K = self._kernel_matrix(X)\n",
    "        \"\"\"\n",
    "        The standard QP solver formulation:\n",
    "        min 1/2 x^T H x + f^T x\n",
    "        s.t.\n",
    "        Ax <=  a\n",
    "        Bx = b\n",
    "        \"\"\"\n",
    "        H = np.einsum(\"i,j,ij->ij\", y, y, K)\n",
    "        f = - np.ones(N, np.double)\n",
    "\n",
    "        I = np.eye(N, dtype=np.double)\n",
    "        A = np.concatenate((I, -I))\n",
    "        a = np.concatenate((np.full(N, self._c), np.zeros(N))).astype(np.double)\n",
    "        \n",
    "        B = y.reshape(1, N)\n",
    "        b = np.zeros(1, np.double)\n",
    "        \n",
    "        # call the QP solver\n",
    "        solution = cvxopt.solvers.qp(*map(cvxopt.matrix, [H, f, A, a, B, b]))\n",
    "\n",
    "        # Lagrange multipliers (the unknown vector 'x' is our alphas)\n",
    "        return np.ravel(solution['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trainer.\n",
    "The following code would generate data which the grounth truth split is x+y = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SVM(Kernel.linear(), 100)\n",
    "test_linear_SVM(clf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVM(Kernel.gaussian(1), 100)\n",
    "test_rbf_SVM(clf, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think your code is correct, then we can move to the real problem. Below are some code examples; you will need to fill in some details, and extend these to any experiments you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " for C in [RANGE FOR C VALUES]:\n",
    "    clf = SVM(Kernel.linear(), C)\n",
    "    clf.fit(X['train'], y['train'].astype('double'))\n",
    "    print(\"C = \", C)\n",
    "    y_hat = clf.predict(X['train'])\n",
    "    print(\"Acc on train: \", np.mean(y_hat == y['train']))\n",
    "    y_hat = clf.predict(X['val'])\n",
    "    print(\"Acc on val: \", np.mean(y_hat == y['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best C, and predict the label for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = BEST VALUE\n",
    "clf = SVM(Kernel.linear(), C)\n",
    "clf.fit(X['train'], y['train'].astype('double'))\n",
    "y_hat = clf.predict(X['test'])\n",
    "save_submission('sub_linear.csv', y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF (Gaussian) kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for C in [RANGE OF C VALUES]:\n",
    "    for gamma in [RANGE OF GAMMA VALUES]:\n",
    "        clf = SVM(Kernel.gaussian(gamma), C)\n",
    "        clf.fit(X['train'], y['train'].astype('double'))\n",
    "        print(\"C = \", C)\n",
    "        print(\"gamma = \", gamma)\n",
    "        y_hat = clf.predict(X['train'])\n",
    "        print(\"Acc on train: \", np.mean(y_hat == y['train']))\n",
    "        y_hat = clf.predict(X['val'])\n",
    "        print(\"Acc on val: \", np.mean(y_hat == y['val']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVM(Kernel.gaussian(BEST GAMMA), BEST C)\n",
    "clf.fit(X['train'], y['train'].astype('double'))y_hat = clf.predict(X['test'])\n",
    "save_submission('sub_rbf.csv', y_hat)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
