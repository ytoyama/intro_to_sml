\documentclass{introtosml}
\nth{3}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\begin{document}

\maketitle

\begin{p}
  \item
    A decision tree can classify linearly separable data.
    A boundary made by such a tree looks like stairs
    approximating $\w^T \x + w_0 = 0$.
    And, in the worst case, its depth is $\lceil \log \frac{N}{2} \rceil + 1$
    because we can balance the tree along $x_1$.

  \item
    A decision tree can classify data points which are not linearly separable
    by separating a space of $\x$ into $N - 1$ thin regions along $x_1$.
    And, in the worst case, its depth is $\lceil \log (N - 1) \rceil$
    balancing

  \item
    Let $M$ be the number of augmented data points.
    \begin{gather*}
      \sum_{i=1}^{N+M} (y_i - \w^T \x_i)^2
          = \sum_{i=1}^{N} (y_i - \w^T \x_i)^2 + \lambda \norm{\w}^2 \\
      \sum_{i=N+1}^{N+M} (y_i - \w^T \x_i)^2 = \lambda \norm{\w}^2
    \end{gather*}
    Let $y_i = 0$ and $\x_i = [0, a, \ldots, a]^T $.
    \begin{gather*}
      \begin{aligned}
        \sum_{i=N+1}^{N+M} a^2 \norm{\w}^2 & = \lambda \norm{\w}^2 \\
        M a^2 \norm{\w}^2 & = \lambda \norm{\w}^2 \\
        M a^2 & = \lambda
      \end{aligned} \\
      \therefore \vec{y}' = \left[\begin{array}{cccc}
        y_1 \\ \vdots \\ y_N \\ 0 \\ \vdots \\ 0
      \end{array}\right],
      X' = \left[\begin{array}{cccc}
        1 & x_{11} & \cdots & x_{1d} \\
        \vdots & \vdots & \vdots & \vdots  \\
        1 & x_{N1} & \cdots & x_{Nd} \\
        0 & a & \vdots & a \\
        \vdots & \vdots & \vdots & \vdots  \\
        0 & a & \vdots & a \\
      \end{array}\right] \\
      \st Ma^2 = \lambda \text{(where $M$ is the number of augmented data points)}
    \end{gather*}

  \item
    \begin{align*}
      \forall i, j, \log \frac{p(c_i|\x)}{p(c_j|\x)} & = \w_{ij} \cdot \x \\
      \log p(c_i|\x) - \log p(c_j|\x) & = \w_{ij} \cdot \x
    \end{align*}
    Let $\log p(c_i|x) = \w_i \cdot \x$. This doesn't break generality
    of the model above because $\w_{ij} = - \w_{ji}$ obviously
    and for all $i$ and $j$ we can pick any $\w_{ij}$
    even if either $\w_i$ or $\w_j$ is fixed.
    \begin{align*}
      \w_i \cdot \x - \w_j \cdot \x & = \w_{ij} \cdot \x \\
      \w_{ij} & = \w_i - \w_j
    \end{align*}
    \begin{align*}
      p(c_i|\x) = e^{\w_{ij} \cdot \x} p(c_j|\x) \\
      p(c_i|\x) = e^{\w_i \cdot \x} e^{- \w_j \cdot \x} p(c_j|\x) \\
      1 = \sum_{i=1}^C e^{\w_i \cdot \x} e^{- \w_j \cdot \x} p(c_j|\x) \\
      p(c_j|\x) = \frac{e^{\w_j \cdot \x}}{\sum_{i=1}^C e^{\w_i \cdot \x}}
    \end{align*}
    \therefore the softmax model corresponds to modeling the log-odds
    between any two classes. \\
    If the number of classes equals 2,
    \begin{align*}
      \frac{e^{\w_j \cdot \x}}{\sum_{i=1}^C e^{\w_i \cdot \x}}
      & = \frac{1}{\sum_{i=1}^C e^{(\w_i - \w_j) \cdot \x}} \\
      & = \sigma((\w_i - \w_j) \cdot \x) \\
      & = \sigma(\vec{v} \cdot \x)
    \end{align*}
    \therefore In the binary case the softmax model is equivalent
    to the logistic regression model.

  \item
    \begin{gather*}
      \begin{aligned}
        L(Y|X; W, \bb)
        & \approx L(y|\x; W, \bb) \\
        & = - \log \hat{p}(y|\x; W, \bb) + \frac{\lambda}{2} \norm{W}^2 \\
        & = - \log \frac{e^{W_y \cdot \x + \bb_y}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            + \frac{\lambda}{2} \norm{W}^2
      \end{aligned} \\
      \begin{aligned}
        \frac{\partial}{\partial W_{ci}} L(y|\x; W, \bb)
        & = - p(y=c) \x_i + \frac{\x_i e^{W_c \cdot \x + \bb_c}}
                                 {\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            + \lambda W_{ci} \\
        & = \left(\frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            - p(y=c)\right) \x_i + \lambda W_{ci}
      \end{aligned} \\
      \begin{aligned}
        \frac{\partial}{\partial \bb_{c}} L(y|\x; W, \bb)
        & = \frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}} - p(y=c)
      \end{aligned} \\
    \end{gather*}
    \therefore The update equasions are the below.
    \begin{gather*}
      W_{ci} \leftarrow W_{ci} - \eta
      \left(\left(\frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            - p(y=c)\right) \x_i + \lambda W_{ci}\right) \\
      \bb_c \leftarrow \bb_c - \eta
      \left( \frac{e^{W_c \cdot \x + \bb_c}}
                  {\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}} - p(y=c)\right)
    \end{gather*}

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\end{document}
