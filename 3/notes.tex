\documentclass{introtosml}
\nth{3}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\begin{document}

\maketitle

\begin{p}
  \item
    A decision tree can classify linearly separable data.
    A boundary made by such a tree looks like stairs
    approximating $\w^T \x + w_0 = 0$.
    And, in the worst case, its depth is
    $\lceil \log \lceil \frac{N}{2} \rceil \rceil + 1$
    because we can separate a space of $\x$
    into $\lceil \frac{N}{2} \rceil$ thin regions
    and balance the tree along $\x_1$.

  \item
    A decision tree can classify data points which are not linearly separable
    by separating a space of $\x$ into $N$ thin regions along $\x_1$.
    And, in the worst case, its depth is $\lceil \log N \rceil$
    when the tree is balanced in the same way as in the problem 1.

  \item

  \item
    \begin{align*}
      \frac{\partial}{\partial \alpha_t} L(H_t, X) & = 0 \\
      \frac{\partial}{\partial \alpha_t} \left(
        e^{-\alpha_t} (1 - \epsilon_t) + e^{\alpha_t} \epsilon_t
      \right) & = 0 \\
      - e^{-\alpha_t} (1 - \epsilon_t) + e^{\alpha_t} \epsilon_t & = 0 \\
      e^{2 \alpha_t} & = \frac{1 - \epsilon_t}{\epsilon_t} \\
      \alpha_t & = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}
    \end{align*}

  \item
    \begin{gather*}
      \begin{aligned}
        L(Y|X; W, \bb)
        & \approx L(y|\x; W, \bb) \\
        & = - \log \hat{p}(y|\x; W, \bb) + \frac{\lambda}{2} \norm{W}^2 \\
        & = - \log \frac{e^{W_y \cdot \x + \bb_y}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            + \frac{\lambda}{2} \norm{W}^2
      \end{aligned} \\
      \begin{aligned}
        \frac{\partial}{\partial W_{ci}} L(y|\x; W, \bb)
        & = - p(y=c) \x_i + \frac{\x_i e^{W_c \cdot \x + \bb_c}}
                                 {\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            + \lambda W_{ci} \\
        & = \left(\frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            - p(y=c)\right) \x_i + \lambda W_{ci}
      \end{aligned} \\
      \begin{aligned}
        \frac{\partial}{\partial \bb_{c}} L(y|\x; W, \bb)
        & = \frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}} - p(y=c)
      \end{aligned} \\
    \end{gather*}
    \therefore The update equasions are the below.
    \begin{gather*}
      W_{ci} \leftarrow W_{ci} - \eta
      \left(\left(\frac{e^{W_c \cdot \x + \bb_c}}{\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}}
            - p(y=c)\right) \x_i + \lambda W_{ci}\right) \\
      \bb_c \leftarrow \bb_c - \eta
      \left( \frac{e^{W_c \cdot \x + \bb_c}}
                  {\sum_{c=1}^C e^{W_c \cdot \x + \bb_c}} - p(y=c)\right)
    \end{gather*}

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\end{document}
