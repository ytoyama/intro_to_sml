\documentclass{introtosml}
\nth{3}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\newcommand{\sumhi}{\sum_{i \st y_i \ne h_{T+1}(\x_i)}}
\newcommand{\W}[1]{W_i^{(#1)}}
\newcommand{\e}[1]{\epsilon_{#1}}
\newcommand{\ett}{\e{T+1}}

\begin{document}

\maketitle

\begin{p}
  \item
    A decision tree can classify linearly separable data.
    A boundary made by such a tree looks like stairs
    approximating $\w^T \x + w_0 = 0$.
    And, in the worst case, its depth is
    $\lceil \log \lceil \frac{N}{2} \rceil \rceil + 1$
    because we can separate a space of $\x$
    into $\lceil \frac{N}{2} \rceil$ thin regions
    and balance the tree along $\x_1$.

  \item
    A decision tree can classify data points which are not linearly separable
    by separating a space of $\x$ into $N$ thin regions along $\x_1$.
    And, in the worst case, its depth is $\lceil \log N \rceil$
    when the tree is balanced in the same way as in the problem 1.

  \item
    \begin{align*}
      \sumhi \W{T+1}
      & = \sumhi \frac{1}{Z} \W{T} e^{- \alpha_{T+1} y_i h_{T+1}(\x_i)} \\
      & = \frac{1}{Z} \sumhi \W{T} e^{\frac{1}{2} \log \frac{1 - \ett}{\ett}} \\
      & = \frac{1}{Z} \sumhi \W{T} \sqrt{\frac{1 - \ett}{\ett}} \\
      & = \frac{1}{Z} \sqrt{\frac{1 - \ett}{\ett}} \sumhi \W{T} \\
      & = \frac{\sqrt{\ett (1 - \ett)}}{Z}
    \end{align*}
    \begin{align*}
      Z
      & = e^{- \alpha_{T+1}} (1 - \ett) e^{\alpha_T} \ett \\
      & = \sqrt{\frac{\ett}{1 - \ett}} (1 - \ett)
          + \sqrt{\frac{1 - \ett}{\ett}} \ett \\
      & = 2 \sqrt{\ett (1 - \ett)}
    \end{align*}
    \begin{gather*}
      \therefore \sumhi \W{T+1} = \frac{1}{2}
    \end{gather*}

  \item
    \begin{align*}
      \frac{\partial}{\partial \alpha_t} L(H_t, X) & = 0 \\
      \frac{\partial}{\partial \alpha_t} \left(
        e^{-\alpha_t} (1 - \e{t}) + e^{\alpha_t} \e{t}
      \right) & = 0 \\
      - e^{-\alpha_t} (1 - \e{t}) + e^{\alpha_t} \e{t} & = 0 \\
      e^{2 \alpha_t} & = \frac{1 - \e{t}}{\e{t}} \\
      \alpha_t & = \frac{1}{2} \log \frac{1 - \e{t}}{\e{t}}
    \end{align*}

  \item

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\end{document}
