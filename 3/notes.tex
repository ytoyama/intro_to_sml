\documentclass{introtosml}
\nth{3}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\begin{document}

\maketitle

\begin{p}
  \item
    A decision tree can classify linearly separable data.
    A boundary made by such a tree looks like stairs
    approximating $\w^T \x + w_0 = 0$.
    And, in the worst case, its depth is
    $\lceil \log \lceil \frac{N}{2} \rceil \rceil + 1$
    because we can separate a space of $\x$
    into $\lceil \frac{N}{2} \rceil$ thin regions
    and balance the tree along $\x_1$.

  \item
    A decision tree can classify data points which are not linearly separable
    by separating a space of $\x$ into $N$ thin regions along $\x_1$.
    And, in the worst case, its depth is $\lceil \log N \rceil$
    when the tree is balanced in the same way as in the problem 1.

  \item

  \item
    \begin{align*}
      \frac{\partial}{\partial \alpha_t} L(H_t, X) & = 0 \\
      \frac{\partial}{\partial \alpha_t} \left(
        e^{-\alpha_t} (1 - \epsilon_t) + e^{\alpha_t} \epsilon_t
      \right) & = 0 \\
      - e^{-\alpha_t} (1 - \epsilon_t) + e^{\alpha_t} \epsilon_t & = 0 \\
      e^{2 \alpha_t} & = \frac{1 - \epsilon_t}{\epsilon_t} \\
      \alpha_t & = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}
    \end{align*}

  \item

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\end{document}
