\documentclass{introtosml}
\nth{3}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\newcommand{\sumhi}[1][T+1]{\sum_{i \st y_i \ne h_{#1}(\x_i)}}
\newcommand{\W}[1]{W_i^{(#1)}}
\newcommand{\e}[1]{\epsilon_{#1}}
\newcommand{\ett}{\e{T+1}}

\newcommand\ywxxi{y_i (\w^T \phi(\x_i) + w_0) - 1 + \xi_i}
\newcommand\loss{
  - \frac{1}{2} \norm{\w}^2 - C \sum_{i=1}^N \xi_i
  + \sum_{i=1}^N \alpha_i \left( y_i (\w^T \phi(\x_i) + w_0) - 1 + \xi_i \right)
  + \sum_{i=1} \mu_i \xi_i}

\begin{document}

\maketitle

\begin{p}
  \item
    A decision tree can classify linearly separable data.
    A boundary made by such a tree looks like stairs
    approximating $\w^T \x + w_0 = 0$.
    And, in the worst case, its depth is
    $\lceil \log \lceil \frac{N}{2} \rceil \rceil + 1$
    because we can separate a space of $\x$
    into $\lceil \frac{N}{2} \rceil$ thin regions
    and balance the tree along $\x_1$.

  \item
    A decision tree can classify data points which are not linearly separable
    by separating a space of $\x$ into $N$ thin regions along $\x_1$.
    And, in the worst case, its depth is $\lceil \log N \rceil$
    when the tree is balanced in the same way as in the problem 1.

  \item
    \begin{align*}
      \sumhi \W{T+1}
      & = \sumhi \frac{1}{Z} \W{T} e^{- \alpha_{T+1} y_i h_{T+1}(\x_i)} \\
      & = \frac{1}{Z} \sumhi \W{T} e^{\frac{1}{2} \log \frac{1 - \ett}{\ett}} \\
      & = \frac{1}{Z} \sumhi \W{T} \sqrt{\frac{1 - \ett}{\ett}} \\
      & = \frac{1}{Z} \sqrt{\frac{1 - \ett}{\ett}} \sumhi \W{T} \\
      & = \frac{\sqrt{\ett (1 - \ett)}}{Z}
    \end{align*}
    \begin{align*}
      Z
      & = e^{- \alpha_{T+1}} (1 - \ett) + e^{\alpha_T} \ett \\
      & = \sqrt{\frac{\ett}{1 - \ett}} (1 - \ett)
          + \sqrt{\frac{1 - \ett}{\ett}} \ett \\
      & = 2 \sqrt{\ett (1 - \ett)}
    \end{align*}
    \begin{gather*}
      \therefore \sumhi \W{T+1} = \frac{1}{2}
    \end{gather*}
    Assume $h_{T+2} = h_{T+1}$.
    \begin{align*}
      \sumhi \W{T+1} & = \frac{1}{2} \\
      \sumhi[T+2] \W{T+1} & = \frac{1}{2} \\
      \e{T+2} & = \frac{1}{2} \\
      \e{T+2} & \ge \frac{1}{2} \contradiction
    \end{align*}
    \therefore $h_{T+2} \ne h_{T+1}$

  \item
    \begin{align*}
      \frac{\partial}{\partial \alpha_t} L(H_t, X) & = 0 \\
      \frac{\partial}{\partial \alpha_t} \left(
        e^{-\alpha_t} (1 - \e{t}) + e^{\alpha_t} \e{t}
      \right) & = 0 \\
      - e^{-\alpha_t} (1 - \e{t}) + e^{\alpha_t} \e{t} & = 0 \\
      e^{2 \alpha_t} & = \frac{1 - \e{t}}{\e{t}} \\
      \alpha_t & = \frac{1}{2} \log \frac{1 - \e{t}}{\e{t}}
    \end{align*}

  \item
    \begin{align*}
      & \min_\w \frac{1}{2} \norm{\w}^2
      + C \sum_{i=1}^N \max \left\{ 0, 1 - y_i (\w^T \phi(\x_i) + w_0) \right\} \\
      \Leftrightarrow & \begin{gathered}
        \max_{\w, \xi} - \frac{1}{2} \norm{\w}^2 - C \sum_{i=1}^N \xi_i \\
        \begin{cases}
          y_i (\w^T \phi(\x_i) + w_0) - 1 + \xi_i \ge 0 \\
          \xi_i \ge 0
        \end{cases}
      \end{gathered}
    \end{align*}
    Using Langrange multipliers,
    \begin{align*}
      \Leftrightarrow & \begin{gathered}
        \min_{\alpha, \mu} \max_{\w, \xi} \loss \\
        \begin{cases}
          \ywxxi \ge 0 \\
          \xi_i \ge 0 \\
          \alpha_i \ge 0 \\
          \mu_i \ge 0 \\
          \alpha_i (\ywxxi) = 0 \\
          \mu_i \xi_i = 0
        \end{cases}
      \end{gathered}
    \end{align*}
    Let $L = \loss$.
    \begin{align*}
      \frac{\partial L}{\partial \w} & = - \w + \sum_{i=1}^N \alpha_i y_i \phi(\x_i) = 0 \\
      \frac{\partial L}{\partial w_0} & = \sum_{i=1}^N \alpha_i y_i = 0 \\
      \frac{\partial L}{\partial \xi_i} & = - C + \alpha_i + \mu_i = 0 \\
    \end{align*}

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\end{document}
