\documentclass{introtosml}
\nth{5}

\newcommand\Ep[1]{E_{p(\vec{x}, y)} \left[ #1 \right]}
\newcommand\fancyb{\beta_{\hat{y}, y}}

\newcommand{\x}{\vec{x}}
\newcommand{\q}{q(c_r = c' | \x)}
\newcommand{\qc}{q(c_r = c | \x)}
\newcommand{\pyx}[1]{p(y = #1 | \x)}
\newcommand{\pyxh}{\pyx{h^*(\x)}}
\newcommand{\pyxc}{\pyx{c}}
\newcommand{\intx}[1]{\int_\vec{x} #1 d\vec{x}}
\newcommand{\R}[1]{R(#1 | \vec{x})}

\newcommand{\w}{\vec{w}}
\newcommand{\bb}{\vec{b}}

\newcommand{\sumhi}[1][T+1]{\sum_{i \st y_i \ne h_{#1}(\x_i)}}
\newcommand{\W}[1]{W_i^{(#1)}}
\newcommand{\e}[1]{\epsilon_{#1}}
\newcommand{\ett}{\e{T+1}}

\newcommand\ywxxi{y_i (\w^T \phi(\x_i) + w_0) - 1 + \xi_i}
\newcommand\loss{
  - \frac{1}{2} \norm{\w}^2 - C \sum_{i=1}^N \xi_i
  + \sum_{i=1}^N \alpha_i \left( y_i (\w^T \phi(\x_i) + w_0) - 1 + \xi_i \right)
  + \sum_{i=1}^N \mu_i \xi_i}
\newcommand\lastloss{\frac{1}{2} \sumij - \sum_{i=1}^N \alpha_i}
\newcommand\sumij{\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \phi(\x_i) \phi(\x_j)}
\newcommand\yphi[2]{y_{#1} y_{#2} \phi(\x_{#1}) \phi(\x_{#2})}

\newcommand\Lnb{\sum_{i=1}^N (y_i - \w \cdot \phi(x_i))^2}
\newcommand\Lnbl{- \Lnb + \lambda (\tau - \sum_{j=1}^d w_j^2)}
\newcommand\pinb[1]{\pi_{#1} \prod_{j=1}^d \theta_{#1 j}^{x_{ij}} (1 - \theta_{#1 j})^{1 - x_{ij}}}



\newcommand\expp[1]{\exp\left(#1\right)}

\newcommand\gauss[1]{\prod_{j=1}^d \frac{1}{\sqrt{2 \pi \sigma_j^2}} \expp{- \frac{(x_j - \mu_{#1,j})^2}{2 \sigma_j^2}}}
\newcommand\cgauss[1]{p(y=#1)\gauss{#1}}
\newcommand\cgaus[1]{p(y=#1) \prod_{j=1}^d \expp{- (x_j - \mu_{#1,j})^2}}
\newcommand\pbias{\frac{p(y=0)}{p(y=1)}}

\begin{document}

\maketitle

\begin{p}
  \item
    \begin{gather}
      \begin{aligned}
        p(y=1|\x)
        & = \frac{p(\x|y=1)p(y=1)}{p(\x)} \\
        & = \frac{p(y=1)}{p(\x)} N(\mu_1, \Sigma_1) \\
        & = \frac{p(y=1)}{p(\x)} \prod_{j=1}^d N(\mu_{1,j}, \sigma_j) \\
        & = \frac{p(y=1)}{p(\x)} \gauss{1} \\
        & = \frac{\cgauss{1}}{\cgauss{0} + \cgauss{1}} \\
        & = \frac{1}{1 + \frac{\cgauss{0}}{\cgauss{1}}} \\
        & = \frac{1}{1 + \pbias \prod_{j=1}^d \expp{(x_j - \mu_{1,j})^2 - (x_j - \mu_{0,j})^2}} \\
        & = \frac{1}{1 +
            \expp{\log \pbias + \sum_{j=1}^d (\mu_{1,j}^2 - \mu_{0,j}^2)
            + 2 \sum_{j=1}^d (\mu_{0,j} - \mu{1,j}) x_j}}
      \end{aligned} \\
      \therefore \begin{cases}
        w_0 = \sum_{j=1}^d (\mu_{1,j}^2 - \mu_{0,j}^2) \\
        \w = 2 (\mu_0 - \mu_1)
      \end{cases}
    \end{gather}

  \item
    Yes, they produce the same classifier because it's a convex optimization problem.

  \item
    Please, see a Jupyter notebook file submitted together.
\end{p}

\begin{thebibliography}{9}
  \bibitem{Bishop} Christopher M. Bishop, Pattern Recognition and Machine Learning
  \bibitem{discussion} Discussion with Tomoki Tsujimura and Bowen Shi
\end{thebibliography}

\end{document}
